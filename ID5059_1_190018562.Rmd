---
title: "ID5059_1_190018562"
author: "Sophia"
date: "02/03/2022"
output: html_document
---

Please find committed changes in my github repository <https://github.com/selsiekiely/ID5059_190018562>.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(readr)
```

Import DataSet containing factors for sales of used cars. This dataset is quite large(1.5GB). We estimate having to reduce the data set quite heavily in order to run models quickly.

```{r}
vehicles <- read_csv("C:/Users/Sophia/ID5059/vehicles.csv")
summary(vehicles)
```


```{r, echo = FALSE}
library(dplyr)
library(tidyr)
```

```{r}
#Use package Amelia to visualise NA values in dataset
library(naniar)

vis_miss(vehicles, warn_large_data = FALSE)
```

Remove unnecessary columns such as ‘id’, ’URL', ‘region_url’, 'VIN', ‘image_url’,'description', 'county' and 'posting_date' because these do not contain metric data that can be utilised in price entry predictions even with feature engineering.

'region', 'lat' and 'long' are removed because they are a feature of location that is metricized in state instead. Their correlations would not work in a random forest model specifically.

'size' removed because kaggle says it is 72% NA

```{r}
vehicles <- vehicles %>% 
  #remove unnecessary columns
  select(-region, -id, -url, -region_url, -VIN, -image_url,-size, -description, -county, -lat, -long, -posting_date)

#get rid of prices that are 0 or NA
vehicles <- vehicles %>% 
  #remove price rows with blanks
  filter(price != 0 & !is.na(price))
```

If we summarise all the variables we can also locate outliers.

```{r}
summary(vehicles$price)
max(vehicles$price)
hist(log(vehicles$price), breaks = 50, main = "Histogram of Used Vehicle Prices", xlab = "Log of Vehicle Price in $")
sum(vehicles$price > 100000)
sum(vehicles$price < 750)
```

Let us remove data that is above 100k$ (log(100000) = 11) in entry price because there are only 655 out of 426880 at this price. They are outliers because only a small subset of buyers represent the ability to pay this much to buy a used car.

We will also drop rows with entry prices less than 750 because these lower prices will be less relevant than drawing correlations from the larger changes in entry prices. We drop 44778 rows.

```{r}
vehicles <- vehicles %>%
  filter(price <= 100000) %>%
  filter(price >= 750)
```

```{r, echo = FALSE}
library(ggplot2)
```


```{r}
#For easier data analysis
vehicles <- as_tibble(vehicles)
```

Years

```{r}
ggplot(vehicles, aes(x = factor(year)), fill = factor(year)) +
  geom_bar() + 
  xlab("Year") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))

#Years is already numeric so no need to change
```

We can see that years below 1970 and above 2021 have an inconsistency. Engineering developments would have influenced the manufacturing of cars and we would like a clear trend in our years and respective counts in order to be able to use the data. The slight upwards fluctuations before 1970 could also mean a fashion trend in car type or popularity in manufacturing methods which are inconsistent with measuring years and progress in car manufacturing. Beyond 2021 we have data which should not exist yet.

```{r}
vehicles <- vehicles %>%
  filter(year > 1995) %>%
  filter(year < 2021)
```

Manufacturer

```{r}
vehicles_manufact <- vehicles %>% 
  group_by(manufacturer) %>%   
  mutate(count_name_occurr = n())

ggplot(vehicles_manufact, aes(x = reorder(factor(manufacturer), -count_name_occurr), fill = factor(manufacturer))) +
  geom_bar(stat = "count") + 
  xlab("Car Manufacturers") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))
```

Let us drop rows where manufacturer is unknown. Median, mode or mean would not suffice for the above distributed data.

```{r}
vehicles <- vehicles %>%
  drop_na(manufacturer)

vehicles$manufacturer <- as.numeric(as.factor(vehicles$manufacturer))
```

Model

```{r}
#There are many model values and we do not attempt to plot

vehicles$model<- as.numeric(as.factor(vehicles$model))
```

Condition

```{r}
#Plot initially condition values
ggplot(vehicles, aes(x = factor(condition), fill = factor(condition))) +
  geom_bar() + 
  xlab("Condition") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))

#group by year so condition can be filled with NAs in correlation to year
vehicles <- vehicles %>%
  dplyr::group_by(year) %>%
  fill(condition, .direction = "downup") %>%
  dplyr::ungroup() %>%
  drop_na(condition)

class(vehicles$condition)

#convert to numeric values
vehicles$condition <- recode(vehicles$condition, "salvaged" = 0, "new" = 1, "like new" = 2, "good" = 3, "fair" = 4, "excellent" = 5)
```

Cylinders

```{r}
class(vehicles$cylinders)
#remove "cylinders" character part of data column
vehicles$cylinders <- gsub("cylinders", "", as.character(vehicles$cylinders))

#replace others to be NAs as well
vehicles$cylinders <- gsub("other", NA, as.character(vehicles$cylinders))

#Group by type to replace with fill() NA cylinder values more accurately. This accounts 
#for drive and cylinder being correlated.
vehicles <- vehicles %>%
  dplyr::group_by(type) %>%
  fill(cylinders, .direction = "downup") %>%
  dplyr::ungroup()

#plot
ggplot(vehicles, aes(x = factor(cylinders), fill = factor(cylinders))) +
  geom_bar() + 
  xlab("State") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))

#make numeric
vehicles$cylinders <- as.numeric(vehicles$cylinders)
```

Fuel

```{r}
class(vehicles$fuel)

#Remove NA rows
vehicles <- vehicles %>%
  drop_na(fuel)

ggplot(vehicles, aes(x = factor(fuel), fill = factor(fuel))) +
  geom_bar() + 
  xlab("Fuel") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))

#make numeric
vehicles$fuel <- as.numeric(as.factor(vehicles$fuel))
```

Odometer

```{r}
hist(log(vehicles$odometer), breaks = 50, main = "Histogram of Odometer", xlab = "Odometer value in log miles")

#We remove odometer values greater than 300000 miles(log(300000) = 12.6)  and less than 400 miles (log(400) = 5.99)

vehicles <- vehicles %>%
  filter(odometer <= 300000) %>%
  filter(odometer >= 400)
```

Transmission

```{r}
#Automatic and Manual are the dominating variables and we make life easier for ourselves by making transmission binary and removing "others"
vehicles <- vehicles %>%
  filter(transmission == "automatic" | transmission == "manual")

#make numeric
vehicles$transmission <- as.numeric(as.factor(vehicles$transmission))
```

Drive

```{r}
vehicles <- vehicles %>%
  drop_na(drive)

ggplot(vehicles, aes(x = factor(drive), fill = factor(drive))) +
  geom_bar() + 
  xlab("Drive") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))

vehicles$drive <- as.numeric(as.factor(vehicles$drive))
```

Type

```{r}
vehicles <- vehicles %>%
  drop_na(type)

ggplot(vehicles, aes(x = factor(type), fill = factor(type))) +
  geom_bar() + 
  xlab("Type") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))

vehicles$type <- as.numeric(as.factor(vehicles$type))
```

Paint Color
```{r}
ggplot(vehicles, aes(x = factor(paint_color), fill = factor(paint_color))) +
  geom_bar() + 
  xlab("State") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))

#Lets replace NAs in color using the following mode function

#Create a function that finds the mode
mode <- function(x){
      ux <- sort(unique(x))
      ux[which.max(tabulate(match(x, ux)))] 
}


vehicles$paint_color[is.na(vehicles$paint_color)] <- mode(vehicles$paint_color)

vehicles$paint_color <- as.numeric(as.factor(vehicles$paint_color))
```

State

```{r}
ggplot(vehicles, aes(x = factor(state), fill = factor(state))) +
  geom_bar() + 
  xlab("State") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))

vehicles$state <- as.numeric(as.factor(vehicles$state))
```


```{r, echo= FALSE}
library(randomForest)
library(ISLR2)
library(tidyverse)
```


```{r}
vehicles_ml <- as.data.frame(vehicles)


vehicles_lm <- lm(price ~ ., data = vehicles, na.action = na.exclude)

plot(vehicles_lm)
#This shows it is not a very ideal model but we make do with it

#Find variable importance from Linear Model
varImp(vehicles_lm)
```
The VarImp() function of the linear model tells us that the most important variables are year, cylinders, fuel and odometer.

We could also try machine learning feature selection to show this, however I cannot make it work.

```{r}
set.seed(8562)

library(mlbench)
library(caret)

#fit_rf=randomForest(price~., data=vehicles, na.action = na.exclude)
#HOWEVER, the above did not work because of allocating a vector of size 1.3 Gb.

# Create an importance based on mean decreasing gini
#importance(fit_rf)

# compare the feature importance with varImp() function
#varImp(fit_rf)
 
# Create a plot of importance scores by random forest
#varImpPlot(fit_rf)
```

Select rows that we are considering as co-variates: Year, Cylinders, Fuel and Odometer. Make a new dataset for the considered covariates:

```{r}
modelled_vehicles <- vehicles %>%
  select(price, year, cylinders, fuel, odometer)
```

## Testing and Training

We split the data into training and test data using a 80/20 split respectively. Seed is set at last
four digits of student ID number.

```{r}
set.seed(8562)

trainInd_model <- sample(1:nrow(modelled_vehicles), round(nrow(modelled_vehicles)*0.80))
testResponse_model <- modelled_vehicles[-trainInd , "price"]
```

```{r}
#fit the random forest model
#model <- randomForest(
#  formula = price ~ .,
#  data = trainInd_model
#)

#display fitted model
#model

#find number of trees that produce lowest test MSE
#which.min(model$mse)

#find RMSE of best model
#sqrt(model$mse[which.min(model$mse)])

#plot the test MSE by number of trees
#plot(model)

#produce variable importance plot
#varImpPlot(model)
```

```{r]}
#model_tuned <- tuneRF(
#               x=trainInd_model[,-1], #define predictor variables
#               y=trainInd_model$price, #define response variable
#               ntreeTry=500,
#               mtryStart=4, 
#               stepFactor=1.5,
#               improve=0.01,
#               trace=FALSE #don't show real-time progress
#               )

#use fitted bagged model to predict price value of new observation

#predict(model, newdata=testResponse_model)
```




