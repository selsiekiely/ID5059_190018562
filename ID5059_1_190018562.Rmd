---
title: "ID5059_1_190018562"
author: "Sophia"
date: "02/03/2022"
output: html_document
---

Please find committed changes in my github repository <https://github.com/selsiekiely/ID5059_190018562>.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Import DataSet containing factors for sales of used cars. This dataset is quite large(1.5GB). We estimate having to reduce the data set quite heavily in order to run models quickly.

```{r}
vehicles <- read.csv("C:/Users/Sophia/ID5059/vehicles.csv")
```

We provide the following data wrangling from reading the general description on KAGGLE:

Remove unnecessary columns such as ‘id’, ’URL', ‘region_url’, 'VIN', ‘image_url’,'description', and 'county' because these do not contain metric data that can be utilised in price entry predictions even with feature engineering.

'lat' and 'long' are removed because they are a feature of location that is metricized in state instead.

'size' removed straight away because kaggle says it is 72% NA.

```{r, echo = FALSE}
library(dplyr)
```

There are no duplicated rows.

```{r}
vehicles <- vehicles %>% 
  select(-id, -url, -region_url, -VIN, -image_url, -description, -county, -lat, -long, -size)

#Replace Blanks with NAs
vehicles[vehicles == ""] <- NA
```

Even after this removals it is necessary to remove rows that contain too many NAs. If we summarise all the variables we can also locate outliers.

```{r}
summary(vehicles$price)
max(vehicles$price)
hist(log(vehicles$price), breaks = 50, main = "Histogram of Used Vehicle Prices", xlab = "Log of Vehicle Price in $")
sum(vehicles$price > 100000)
sum(vehicles$price < 750)
```

Let us remove data that is above 100k$ (log(100000) = 11) in entry price because there are only 655 out of 426880 at this price. They are outliers because only a small subset of buyers represent the ability to pay this much to buy a used car.

We will also drop rows with entry prices less than 750 because these lower prices will be less relevant than drawing correlations from the larger changes in entry prices. We drop 44778 rows.

```{r}
vehicles <- vehicles %>%
  filter(price <= 100000) %>%
  filter(price >= 750)
```

```{r, echo = FALSE}
library(ggplot2)
```

```{r}
ggplot(vehicles, aes(x = price)) +
  geom_density() +
  facet_wrap(~condition)
```

We can see here that despite NA values holding no whereabouts to their condition, we can see they still follow the same trend in price and it is easy to assume imputes via modal condition.

We will now see the columns with the number of NAs as a percentage

```{r}
colSums(is.na(vehicles))/nrow(vehicles)*100
```

We will now remove columns condition, cylinders, drive, type and paint color because these contain NAs greater than 20%. Contradiction from earlier when we discussed Condition NAs being easily imputable, however we want to make it easy for ourselves and impute as least values as possible.

```{r}
vehicles <- vehicles %>% 
  select(-condition, -cylinders, -drive, -type, -paint_color)
```



```{r}
summary(vehicles$manufacturer)

ggplot(vehicles, aes(x = factor(manufacturer), fill = factor(manufacturer))) +
  geom_bar() + 
  xlab("Car Manufacturers") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))
```

```{r}
ggplot(vehicles, aes(x = factor(state), fill = factor(state))) +
  geom_bar() + 
  xlab("State") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))
```

```{r}
ggplot(vehicles, aes(x = factor(year), fill = factor(year))) +
  geom_bar() + 
  xlab("Year") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))
```

We will "impute" using the modal; the most common value in that column. This is for non-numeric, character data. The modal can be imputed via the following code;

```{r}

#Create an index for columns that contain non-numeric data (e.g. manufacturer)
karacter <- !sapply(vehicles, is.numeric)

#Create a function that finds the mode
mode <- function(x){
      ux <- sort(unique(x))
      ux[which.max(tabulate(match(x, ux)))] 
}

#Replace the NAs in character columns with the most frequent value
vehicles[karacter] <- lapply(vehicles[karacter], function(x)
              replace(x, is.na(x), mode(x[!is.na(x)])))

```

Now for numeric variables, we can still use the introduced mode function. 

```{r}
vehicles$year[is.na(vehicles$year)] <- mode(vehicles$year)
vehicles$odometer[is.na(vehicles$odometer)] <- mode(vehicles$odometer)
```

We will definitely use state as one of the co-variates because it is a full column. However, we will need to change the variables to be numeric to account for placement in the model.

```{r}
sapply(vehicles, class)
vehicles$year <- as.numeric(vehicles$year)
vehicles$odometer <- as.numeric(vehicles$odometer)
vehicles$region <- as.numeric(as.character(vehicles$region))
as.numeric(vehicles$region, na.rm = TRUE)
```

```{r}

#Make all entries numeric
#for(i in 1:ncol(vehicles)) {       
#  vehicles[ , i] <- as.numeric(as.character(vehicles[ , i]))
#}

```


## Testing and Training

We split the data into training and test data using a 80/20 split respectively. Seed is set at last
four digits of student ID number.

```{r}
set.seed(8562)
  
# create an index for separating out test and training data 
  
trainInd <- sample(1:nrow(vehicles), round(nrow(vehicles)*0.80))
testResponse <- vehicles[-trainInd , "price"] 

```

Before modelling, lets choose co-variates by comparing using machine learning feature selection;

```{r}
# ensure results are repeatable

set.seed(7)
# load the library
library(mlbench)
library(caret)

#prepare training scheme
#control <- trainControl(method="repeatedcv", number=10, repeats=3)

# train the model
#model <- train(price~., data=vehicles, method="lvq", preProcess="scale", trControl=control)
# estimate variable importance
#importance <- varImp(model, scale=FALSE)
# summarize importance
#print(importance)
# plot importance
#plot(importance)
```

```{r}
#Fitting a linear model with log of price because price is large
vehicles_lm <- lm(log(price) ~ year + manufacturer + transmission + state, data = vehicles, na.action = na.exclude)

#na.exclude allows the model to run because rows with NAs are excluded. However, this does make it dodgy.

summary(vehicles_lm)
```

```{r}
sum(resid(vehicles_lm)^2)
```

The residuals indicate an ok model because max and min are similar magnitude and suggest symmetry. The p value is significantly lower than 0.01 also suggesting a good fit.

```{r}
#Fitting a linear model with log of price because price is large
#vehicles_lm <- lm(log(price) ~ manufacturer + model + fuel + state, data = vehicles, na.action = na.exclude)

#summary(vehicles_lm)
```
However for this one we received an error message for too large data.

```{r}
#Fitting a linear model with log of price because price is large
#vehicles_lm <- lm(log(price) ~ year + manufacturer + model + state, data = vehicles, na.action = #na.exclude)

#summary(vehicles_lm)
```

Again, we received an error.

```{r}
#Fitting a linear model with log of price because price is large
#vehicles_lm <- lm(log(price) ~ year + manufacturer + model + fuel, data = vehicles, na.action = #na.exclude)

#summary(vehicles_lm)
```

Same error message here.

```{r}
#Fitting a linear model with log of price because price is large
vehicles_lm2 <- lm(log(price) ~ year + manufacturer + transmission + fuel, data = vehicles, na.action = na.exclude)

summary(vehicles_lm)
```

The residuals indicate an ok model because max and min are similar magnitude and suggest symmetry. The p value is significantly lower than 0.01 also suggesting a good fit. 

```{r}
sum(resid(vehicles_lm2)^2)
```

This is quite high for residual error summing. However, it is the same training error as before.

```{r}
library(rpart)
  
# the CP = 0 here means there will be no pruning on the basis of the cross-valdiation - we grow a big tree for pruning later
#fit <- rpart(price ~ year + manufacturer + transmission + state, data = vehicles, na.action = na.exclude, method="class", control = rpart.control(cp=0))
```

We attempted to fit the above model. However, we cannot allocate vector of size 1.6Gb.


We will train on 80% of our data and then test on 20% of the data.

```{r, echo= FALSE}
library(randomForest)
library(ISLR2)
library(tidyverse)
```

```{r}
#vehiclesBag <- randomForest(price ~  year + manufacturer + transmission + state , data = vehicles, subset = trainInd, mtry = 4, ntree = 25)

 #predict to the test data 
#bagPred <- predict(vehiclesBag , newdata = vehicles[-trainInd , ])
  
# our test MSE
#mean((bagPred - testResponse)^2)
```

We receive an error because there are missing values in object price according to the error bar.

```{r}
library(gbm)

#vehiclesBoost <- gbm(price ~ year + as.factor(manufacturer) + as.factor(transmission) + as.factor(state), data = vehicles[trainInd, ], distribution = "gaussian", n.trees = 5000, cv.folds = 5, interaction.depth = 4, shrinkage = 0.001, verbose = F)

# variable importances
#summary(vehiclesBoost)
```

We receive a base::try(x,silent = TRUE) : object 'x' not found error.

