---
title: "ID5059_1_190018562"
author: "Sophia"
date: "02/03/2022"
output: html_document
---

Please find committed changes in my github repository <https://github.com/selsiekiely/ID5059_190018562>.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Import DataSet containing factors for sales of used cars.

```{r}
vehicles <- read.csv("C:/Users/Sophia/ID5059/vehicles.csv")
head(vehicles)
colnames(vehicles)
```

We provide the following data wrangling from reading the general description on KAGGLE:

Remove unnecessary columns such as ‘id’, ’URL', ‘region_url’, 'VIN', ‘image_url’,'description', and 'county' because these do not contain metric data that can be utilised in price entry predictions even with feature engineering.

'lat' and 'long' are removed because they are a feature of location that is metricized in state instead.

'size' removed because it is 72% NA.

```{r}
library(dplyr)
vehicles <- vehicles %>% 
  select(-id, -url, -region_url, -VIN, -image_url, -description, -county, -lat, -long, -size)
```

Even after this removals it is necessary to remove rows that contain too many NAs. If we summarise all the variables we can also locate outliers.

```{r}
summary(vehicles$price)
max(vehicles$price)
hist(log(vehicles$price), main = "Histogram of Used Vehicle Prices", xlab = "log(Vehicle Price)")
```

Let us remove data that is above 100km (log(100000) = 11) in entry price because these values hinder a model being made on entry price. They are clearly out-liers because only a small subset of buyers represent the ability to pay this much to buy a used car.

```{r}
vehicles <- vehicles %>%
  filter(price <= 100000)
```



```{r}
library(ggplot2)
ggplot(vehicles, aes(x = factor(manufacturer), fill = factor(manufacturer))) +
  geom_bar() + 
  xlab("Car Manufacturers") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))
```

```{r}
ggplot(vehicles, aes(x = factor(state), fill = factor(state))) +
  geom_bar() + 
  xlab("State") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))
```
We will definitely use state as one of the covariates because it is a full column. However, we will need to change the variables to be numeric to account for placement in the model.

```{r}
ggplot(vehicles, aes(x = price)) +
  geom_density() +
  facet_wrap(~condition)
```

We can see here that despite NA values holding no whereabouts to their condition, we can see they still follow the same trend in price and it is easy to assume imputes via modal condition.

```{r}
#Make all entries numeric
for(i in 1:ncol(vehicles)) {       # for-loop over columns
  vehicles[ , i] <- as.numeric(as.character(vehicles[ , i]))
}

#vehicles_lm <- lm(price ~ year + condition + manufacturer + odometer, data = vehicles, na.action = #na.omit)

```

Now we have decided on co-variants there are still some NA values to be replaced. We will "impute" using the modal; the most common value in that column. This is for non-numeric, character data. The modal can be imputed via the following code;

```{r}

#Create an index for columns that contain non-numeric data (e.g. manufacturer)
karacter <- !sapply(vehicles, is.numeric)

#Create a function that finds the mode
mode <- function(x){
      ux <- sort(unique(x))
      ux[which.max(tabulate(match(x, ux)))] 
}

#Replace the NAs in character columns with the most frequent value
vehicles[karacter] <- lapply(vehicles[karacter], function(x)
              replace(x, is.na(x), mode(x[!is.na(x)])))

```

## Testing and Training

We split the data into training and test data using a 80/20 split respectively. Seed is set at last
four digits of student ID number.

```{r}
set.seed(8562)
  
# create an index for separating out test and training data (50% split) 
  
trainInd <- sample(1:nrow(vehicles), round(nrow(vehicles)*0.80))
testResponse <- vehicles[-trainInd , "price"]
```

We will train on 80% of our data and then test on 20% of the data.

```{r}
library(randomForest)
library(ISLR2)
library(tidyverse)
```

```{r}
vehiclesBag <- randomForest(price ~ ., data = vehicles, subset = trainInd, mtry = 12, ntree = 25)

# predict to the test data 
bagPred <- predict(vehiclesBag , newdata = vehicles[-trainInd , ])
  
# our test MSE
mean((bagPred - testResponse)^2)
```
We receive an error because there are missing values in object.

```{r}
Price <- training_set$price
Region <- as.integer(as.factor(training_set$region))
Year <- training_set$year
par(mfrow=c(3,2))
hist(Price)
boxplot(Price, horizontal=TRUE)
hist(Region)
boxplot(Region, horizontal=TRUE)
hist(Year)
boxplot(Year, horizontal=TRUE)
```

Data Wrangle for purposes of removing "NA"'S and fuller elements

```{r}
```