---
title: "ID5059_1_190018562"
author: "Sophia"
date: "02/03/2022"
output: html_document
---

Please find committed changes in my github repository <https://github.com/selsiekiely/ID5059_190018562>.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Import DataSet containing factors for sales of used cars.

```{r}
vehicles <- read.csv("C:/Users/Sophia/ID5059/vehicles.csv")
```

We provide the following data wrangling from reading the general description on KAGGLE:

Remove unnecessary columns such as ‘id’, ’URL', ‘region_url’, 'VIN', ‘image_url’,'description', and 'county' because these do not contain metric data that can be utilised in price entry predictions even with feature engineering.

'lat' and 'long' are removed because they are a feature of location that is metricized in state instead.

'size' removed straight away because kaggle says it is 72% NA.

```{r, echo = FALSE}
library(dplyr)
```

```{r}
vehicles <- vehicles %>% 
  select(-id, -url, -region_url, -VIN, -image_url, -description, -county, -lat, -long, -size)

#Replace Blanks with NAs
vehicles[vehicles == ""] <- NA
```

Even after this removals it is necessary to remove rows that contain too many NAs. If we summarise all the variables we can also locate outliers.

```{r}
summary(vehicles$price)
max(vehicles$price)
hist(vehicles$price, breaks = 50, main = "Histogram of Used Vehicle Prices", xlab = "Vehicle Price in $")
sum(vehicles$price > 100000)
sum(vehicles$price < 750)
```

Let us remove data that is above 100k$ in entry price because there are only 655 out of 426880 at this price. They are outliers because only a small subset of buyers represent the ability to pay this much to buy a used car.

We will also drop rows with entry prices less than 750 because these lower prices will be less relevant than drawing correlations from the larger changes in entry prices. We drio 44778 rows.

```{r}
vehicles <- vehicles %>%
  filter(price <= 100000) %>%
  filter(price >= 750)
```

We will now see the columns with the number of NAs as a percentage

```{r}
colSums(is.na(vehicles))/nrow(vehicles)*100
```

We will now remove columns condition, cylinders, drive, type and paint color because these contain NAs greater than 20%.

```{r}
vehicles <- vehicles %>% 
  select(-condition, -cylinders, -drive, -type, -paint_color)
```

```{r, echo = FALSE}
library(ggplot2)
```

```{r}
summary(vehicles$region)
```

```{r}
summary(vehicles$manufacturer)

ggplot(vehicles, aes(x = factor(manufacturer), fill = factor(manufacturer))) +
  geom_bar() + 
  xlab("Car Manufacturers") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))
```

```{r}
ggplot(vehicles, aes(x = factor(state), fill = factor(state))) +
  geom_bar() + 
  xlab("State") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))
```

We will definitely use state as one of the covariates because it is a full column. However, we will need to change the variables to be numeric to account for placement in the model.

```{r}
ggplot(vehicles, aes(x = price)) +
  geom_density() +
  facet_wrap(~condition)
```

We can see here that despite NA values holding no whereabouts to their condition, we can see they still follow the same trend in price and it is easy to assume imputes via modal condition.

```{r}
#Make all entries numeric
for(i in 1:ncol(vehicles)) {       # for-loop over columns
  vehicles[ , i] <- as.numeric(as.character(vehicles[ , i]))
}

#vehicles_lm <- lm(price ~ year + condition + manufacturer + odometer, data = vehicles, na.action = #na.omit)

```

Now we have decided on co-variants there are still some NA values to be replaced. We will "impute" using the modal; the most common value in that column. This is for non-numeric, character data. The modal can be imputed via the following code;

```{r}

#Create an index for columns that contain non-numeric data (e.g. manufacturer)
karacter <- !sapply(vehicles, is.numeric)

#Create a function that finds the mode
mode <- function(x){
      ux <- sort(unique(x))
      ux[which.max(tabulate(match(x, ux)))] 
}

#Replace the NAs in character columns with the most frequent value
vehicles[karacter] <- lapply(vehicles[karacter], function(x)
              replace(x, is.na(x), mode(x[!is.na(x)])))

```

## Testing and Training

We split the data into training and test data using a 80/20 split respectively. Seed is set at last
four digits of student ID number.

```{r}
set.seed(8562)
  
# create an index for separating out test and training data 
  
trainInd <- sample(1:nrow(vehicles), round(nrow(vehicles)*0.80))
testResponse <- vehicles[-trainInd , "price"]
```

We will train on 80% of our data and then test on 20% of the data.

```{r, echo= FALSE}
library(randomForest)
library(ISLR2)
library(tidyverse)
```

```{r}
#vehiclesBag <- randomForest(price ~ ., data = vehicles, subset = trainInd, mtry = 12, ntree = 25)

# predict to the test data 
#bagPred <- predict(vehiclesBag , newdata = vehicles[-trainInd , ])
  
# our test MSE
#mean((bagPred - testResponse)^2)
```
We receive an error because there are missing values in object.



Data Wrangle for purposes of removing "NA"'S and fuller elements

```{r}
```