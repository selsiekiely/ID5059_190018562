---
title: "ID5059_1_190018562"
author: "Sophia"
date: "02/03/2022"
output: html_document
---

Please find committed changes in my github repository <https://github.com/selsiekiely/ID5059_190018562>.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(readr)
```

Import DataSet containing factors for sales of used cars. This dataset is quite large(1.5GB). We estimate having to reduce the data set quite heavily in order to run models quickly.

```{r}
vehicles <- read_csv("C:/Users/Sophia/ID5059/vehicles.csv")
summary(vehicles)
```


```{r, echo = FALSE}
library(dplyr)
library(tidyr)
```

```{r}
#Use package Amelia to visualise NA values in dataset
library(naniar)

vis_miss(vehicles, warn_large_data = FALSE)
```

Remove unnecessary columns such as ‘id’, ’URL', ‘region_url’, 'VIN', ‘image_url’,'description', 'county' and 'posting_date' because these do not contain metric data that can be utilised in price entry predictions even with feature engineering.

'region', 'lat' and 'long' are removed because they are a feature of location that is metricized in state instead. Their correlations would not work in a random forest model specifically.

'size' removed because kaggle says it is 72% NA

```{r}
vehicles <- vehicles %>% 
  #remove unnecessary columns
  select(-region, -id, -url, -region_url, -VIN, -image_url,-size, -description, -county, -lat, -long, -posting_date)

#get rid of prices that are 0 or NA
vehicles <- vehicles %>% 
  #remove price rows with blanks
  filter(price != 0 & !is.na(price))
```

If we summarise all the variables we can also locate outliers.

```{r}
summary(vehicles$price)
max(vehicles$price)
hist(log(vehicles$price), breaks = 50, main = "Histogram of Used Vehicle Prices", xlab = "Log of Vehicle Price in $")
sum(vehicles$price > 100000)
sum(vehicles$price < 750)
```

Let us remove data that is above 100k$ (log(100000) = 11) in entry price because there are only 655 out of 426880 at this price. They are outliers because only a small subset of buyers represent the ability to pay this much to buy a used car.

We will also drop rows with entry prices less than 750 because these lower prices will be less relevant than drawing correlations from the larger changes in entry prices. We drop 44778 rows.

```{r}
vehicles <- vehicles %>%
  filter(price <= 100000) %>%
  filter(price >= 750)
```

```{r, echo = FALSE}
library(ggplot2)
```


```{r}
#For easier data analysis
vehicles <- as_tibble(vehicles)

#remove duplicated rows
vehicles <- vehicles %>% distinct()
```

Years

```{r}
ggplot(vehicles, aes(x = factor(year)), fill = factor(year)) +
  geom_bar() + 
  xlab("Year") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))
```

We can see that years below 1970 and above 2021 have an inconsistency. Engineering developments would have influenced the manufacturing of cars and we would like a clear trend in our years and respective counts in order to be able to use the data. The slight upwards fluctuations before 1970 could also mean a fashion trend in car type or popularity in manufacturing methods which are inconsistent with measuring years and progress in car manufacturing. Beyond 2021 we have data which should not exist yet.

```{r}
vehicles <- vehicles %>%
  filter(year > 1995) %>%
  filter(year < 2021)
```

Manufacturer

```{r}
vehicles_manufact <- vehicles %>% 
  group_by(manufacturer) %>%   
  mutate(count_name_occurr = n())

ggplot(vehicles_manufact, aes(x = reorder(factor(manufacturer), -count_name_occurr), fill = factor(manufacturer))) +
  geom_bar(stat = "count") + 
  xlab("Car Manufacturers") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))
```

Let us drop rows where manufacturer is unknown. Median, mode or mean would not suffice for the above distributed data.

```{r}
vehicles <- vehicles %>%
  drop_na(manufacturer)
```

Model

```{r}
vehicles_model <- vehicles %>% 
  group_by(model) %>%   
  mutate(count_name_occurr = n())

ggplot(vehicles_model, aes(x = reorder(factor(model), -count_name_occurr), fill = factor(model))) +
  geom_bar(stat = "count") + 
  xlab("Car Models") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))
```

Condition

```{r}
#Plot initially condition values
ggplot(vehicles, aes(x = factor(condition), fill = factor(condition))) +
  geom_bar() + 
  xlab("Condition") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))

#group by year so condition can be filled with NAs in correlation to year
vehicles <- vehicles %>%
  dplyr::group_by(year) %>%
  fill(condition, .direction = "downup") %>%
  dplyr::ungroup() %>%
  drop_na(condition)

class(vehicles$condition)

#convert to numeric values
vehicles$condition <- recode(vehicles$condition, "salvaged" = 0, "new" = 1, "like new" = 2, "good" = 3, "fair" = 4, "excellent" = 5)
```

Cylinders

```{r}
class(vehicles$cylinders)
#remove "cylinders" character part of data column
vehicles$cylinders <- gsub("cylinders", "", as.character(vehicles$cylinders))

#replace others to be NAs as well
vehicles$cylinders <- gsub("other", NA, as.character(vehicles$cylinders))

#Group by type to replace with fill() NA cylinder values more accurately. This accounts 
#for drive and cylinder being correlated.
vehicles <- vehicles %>%
  dplyr::group_by(type) %>%
  fill(cylinders, .direction = "downup") %>%
  dplyr::ungroup()

#plot
ggplot(vehicles, aes(x = factor(cylinders), fill = factor(cylinders))) +
  geom_bar() + 
  xlab("State") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))

#make numeric
vehicles$cylinders <- as.numeric(vehicles$cylinders)
```

Fuel

```{r}
class(vehicles$fuel)

#Remove NA rows
vehicles <- vehicles %>%
  drop_na(fuel)

ggplot(vehicles, aes(x = factor(fuel), fill = factor(fuel))) +
  geom_bar() + 
  xlab("Fuel") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))

#make numeric
vehicles$fuel <- as.numeric(as.factor(vehicles$fuel))
```

Odometer

```{r}
hist(log(vehicles$odometer), breaks = 50, main = "Histogram of Odometer", xlab = "Odometer value in log miles")

#We remove odometer values greater than 300000 miles(log(300000) = 12.6)  and less than 400 miles (log(400) = 5.99)

vehicles <- vehicles %>%
  filter(odometer <= 300000) %>%
  filter(odometer >= 400)
```

Transmission

```{r}
#Automatic and Manual are the dominating variables and we make life easier for ourselves by making transmission binary and removing "others"
vehicles <- vehicles %>%
  filter(transmission == "automatic" | transmission == "manual")

#make numeric
vehicles$transmission <- as.numeric(as.factor(vehicles$transmission))
```

Drive

```{r}
vehicles <- vehicles %>%
  drop_na(drive)

ggplot(vehicles, aes(x = factor(drive), fill = factor(drive))) +
  geom_bar() + 
  xlab("Drive") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))

vehicles$drive <- as.numeric(as.factor(vehicles$drive))
```

Type

```{r}
vehicles <- vehicles %>%
  drop_na(type)

ggplot(vehicles, aes(x = factor(type), fill = factor(type))) +
  geom_bar() + 
  xlab("Type") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))
```

Paint Color
```{r}
ggplot(vehicles, aes(x = factor(paint_color), fill = factor(paint_color))) +
  geom_bar() + 
  xlab("State") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))

#Lets replace NAs in color using the following mode function

#Create a function that finds the mode
mode <- function(x){
      ux <- sort(unique(x))
      ux[which.max(tabulate(match(x, ux)))] 
}


vehicles$paint_color[is.na(vehicles$paint_color)] <- mode(vehicles$paint_color)

vehicles$paint_color <- as.numeric(as.factor(vehicles$paint_color))
```

State

```{r}
ggplot(vehicles, aes(x = factor(state), fill = factor(state))) +
  geom_bar() + 
  xlab("State") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, size = 4))

vehicles$state <- as.character(as.factor(vehicles$state))

vehicles$state <- as.numeric(as.factor(vehicles$state))
```


## Testing and Training

We split the data into training and test data using a 80/20 split respectively. Seed is set at last
four digits of student ID number.

```{r}
set.seed(8562)
  
# create an index for separating out test and training data 
  
trainInd <- sample(1:nrow(vehicles), round(nrow(vehicles)*0.80))
testResponse <- vehicles[-trainInd , "price"] 

```

Before modelling, lets choose co-variates by comparing using machine learning feature selection;

```{r}
# ensure results are repeatable

set.seed(7)
# load the library
library(mlbench)
library(caret)

#prepare training scheme
#control <- trainControl(method="repeatedcv", number=10, repeats=3)

# train the model
#model <- train(price~., data=vehicles, method="lvq", preProcess="scale", trControl=control)
# estimate variable importance
#importance <- varImp(model, scale=FALSE)
# summarize importance
#print(importance)
# plot importance
#plot(importance)
```

```{r}
#Fitting a linear model with log of price because price is large
vehicles_lm <- lm(log(price) ~ year + manufacturer + transmission + state, data = vehicles, na.action = na.exclude)

#na.exclude allows the model to run because rows with NAs are excluded. However, this does make it dodgy.

summary(vehicles_lm)
```

```{r}
sum(resid(vehicles_lm)^2)
```

The residuals indicate an ok model because max and min are similar magnitude and suggest symmetry. The p value is significantly lower than 0.01 also suggesting a good fit.

```{r}
#Fitting a linear model with log of price because price is large
#vehicles_lm <- lm(log(price) ~ manufacturer + model + fuel + state, data = vehicles, na.action = na.exclude)

#summary(vehicles_lm)
```
However for this one we received an error message for too large data.

```{r}
#Fitting a linear model with log of price because price is large
#vehicles_lm <- lm(log(price) ~ year + manufacturer + model + state, data = vehicles, na.action = #na.exclude)

#summary(vehicles_lm)
```

Again, we received an error.

```{r}
#Fitting a linear model with log of price because price is large
#vehicles_lm <- lm(log(price) ~ year + manufacturer + model + fuel, data = vehicles, na.action = #na.exclude)

#summary(vehicles_lm)
```

Same error message here.

```{r}
#Fitting a linear model with log of price because price is large
vehicles_lm2 <- lm(log(price) ~ year + manufacturer + transmission + fuel, data = vehicles, na.action = na.exclude)

summary(vehicles_lm)
```

The residuals indicate an ok model because max and min are similar magnitude and suggest symmetry. The p value is significantly lower than 0.01 also suggesting a good fit. 

```{r}
sum(resid(vehicles_lm2)^2)
```

This is quite high for residual error summing. However, it is the same training error as before.

```{r}
library(rpart)
  
# the CP = 0 here means there will be no pruning on the basis of the cross-valdiation - we grow a big tree for pruning later
#fit <- rpart(price ~ year + manufacturer + transmission + state, data = vehicles, na.action = na.exclude, method="class", control = rpart.control(cp=0))
```

We attempted to fit the above model. However, we cannot allocate vector of size 1.6Gb.


We will train on 80% of our data and then test on 20% of the data.

```{r, echo= FALSE}
library(randomForest)
library(ISLR2)
library(tidyverse)
```

```{r}
#vehiclesBag <- randomForest(price ~  year + manufacturer + transmission + state , data = vehicles, subset = trainInd, mtry = 4, ntree = 25)

 #predict to the test data 
#bagPred <- predict(vehiclesBag , newdata = vehicles[-trainInd , ])
  
# our test MSE
#mean((bagPred - testResponse)^2)
```

We receive an error because there are missing values in object price according to the error bar.

```{r}
library(gbm)

#vehiclesBoost <- gbm(price ~ year + as.factor(manufacturer) + as.factor(transmission) + as.factor(state), data = vehicles[trainInd, ], distribution = "gaussian", n.trees = 5000, cv.folds = 5, interaction.depth = 4, shrinkage = 0.001, verbose = F)

# variable importances
#summary(vehiclesBoost)
```

We receive a base::try(x,silent = TRUE) : object 'x' not found error.

